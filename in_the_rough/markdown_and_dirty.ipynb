{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2225ab-5157-4f22-8e11-26414e6d5151",
   "metadata": {},
   "source": [
    "Food for Thought "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73497160-a804-4d3b-bbc9-575efad77d23",
   "metadata": {},
   "source": [
    "Scrubbing / Cleaning Your Data\n",
    "Clean up on column 5! This phase of the pipeline should require the most time and effort. Because the results and output of your machine learning model is only as good as what you put into it. Basically, garbage in garbage out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637b3fb-b272-49a9-840e-964294b25554",
   "metadata": {},
   "source": [
    "https://bookdown.org/max/FES/encoding-predictors-with-many-categories.html\n",
    "\n",
    "5.2 Encoding Predictors with Many Categories\n",
    "\n",
    "If there are  C categories, what happens when C becomes very large? For example, ZIP code in the United States may be an important predictor for outcomes that are affected by a geographic component. There are more than 40K possible ZIP codes and, depending on how the data are collected, this might produce an overabundance of dummy variables (relative to the number of data points). As mentioned in the previous section, this can cause the data matrix to be overdetermined and restrict the use of certain models. Also, ZIP codes in highly populated areas may have a higher rate of occurrence in the data, leading to a “long tail” of locations that are infrequently observed.\n",
    "\n",
    "Prior to producing dummy variables, one might consider determining which (if any) of these variables are near-zero variance predictors or have the potential to have near zero variance during the resampling process. These are predictors that have few unique values (such as two values for binary dummy variables) and occur infrequently in the data (Kuhn and Johnson 2013).\n",
    "                        -Note, this is exactly why I need to include Los Angeles, since it makes up more than half of the df-\n",
    "                        \n",
    "Blah. \n",
    "\n",
    "Categorical predictors can take a variety of forms in the data that is to be modeled. With the exception of tree-based models, categorical predictors must first be converted to numeric representations to enable other models to use the information . The most simplistic feature engineering technique for a categorical predictor is to convert each category to a separate binary dummy predictor. But even this basic conversion comes with a primary caution that some models require one fewer dummy predictors than the number of categories. Creating dummy predictors may not be the most effective way of extracting predictive information from a categorical predictor. If, for instance, the predictor has ordered categories, then other techniques such as linear or polynomial contrasts may be better related to the outcome.\n",
    "\n",
    "Done well, feature engineering for categorical predictors can unlock important predictive information related to the outcome. \n",
    "\n",
    "8.1 Understanding the Nature and Severity of Missing Information\n",
    "\n",
    "As illustrated throughout this book, visualizing data are an important tool for guiding us towards implementing appropriate feature engineering techniques. The same principle holds for understanding the nature and severity of missing information throughout the data. Visualizations as well as numeric summaries are the first step in grasping the challenge of missing information in a data set. For small to moderate data (100’s of samples and 100’s of predictors), several techniques are available that allow the visualization of all of the samples and predictors simultaneously for understanding the degree and location of missing predictor values. When the number of samples or predictors become large, the missing information must first be appropriately condensed and then visualized. In addition to visual summaries, numeric summaries are a valuable tool for diagnosing the nature and degree of missingness.\n",
    "\n",
    "Visualizing Missing Information\n",
    "\n",
    "When the training set has a moderate number of samples and predictors, a simple method to visualize the missing information is with a heatmap.\n",
    "\n",
    "When the data has a large number of samples or predictors, using heatmaps or co-occurrence plots are much less effective for understanding missingness patterns and characteristics. In this setting, the missing information must first be condensed prior to visualization. Principal component analysis (PCA) was first illustrated in Section 6.3 as a dimension reduction technique.\n",
    "\n",
    "8.3 Deletion of Data\n",
    "\n",
    "When it is desirable to use models that are intolerant to missing data, then the missing values must be extricated from the data. The simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values. However, one must carefully consider a number of aspects of the data prior to taking this approach. For example, missing values could be eliminated by removing all predictors that contain at least one missing value. Similarly, missing values could be eliminated by removing all samples with any missing values. Neither of these approaches will be appropriate for all data as can be inferred from the “No Free Lunch” theorem. For some data sets, it may be true that particular predictors are much more problematic than others; by removing these predictors, the missing data issue is resolved. For other data sets, it may be true that specific samples have consistently missing values across the predictors; by removing these samples, the missing data issue is likewise resolved. In practice, however, specific predictors and specific samples contain a majority of the missing information.\n",
    "\n",
    "Another important consideration is the intrinsic value of samples as compared to predictors. When it is difficult to obtain samples or when the data contain a small number of samples (i.e., rows), then it is not desirable to remove samples from the data. In general, samples are more critical than predictors and a higher priority should be placed on keeping as many as possible. Given that a higher priority is usually placed on samples, an initial strategy would be to first identify and remove predictors that have a sufficiently high amount of missing data. Of course, predictor(s) in question that are known to be valuable and/or predictive of the outcome should not be removed. Once the problematic predictors are removed, then attention can focus on samples that surpass a threshold of missingness.\n",
    "\n",
    "Besides throwing away data, the main concern with removing samples (rows) of the training set is that it might bias the model that relates the predictors to the outcome. A classic example stems from medical studies. In these studies, a subset of patients are randomly assigned to the current standard of care treatment while another subset of patients are assigned to a new treatment. The new treatment may induce an adverse effect for some patients, causing them to drop out of the study thus inducing missing data for future clinical visits. This kind of missing data is clearly not missing at random and the elimination of these data from an analysis would falsely measure the outcome to be better than it would have if their unfavorable results had been included. That said, Allison (2001) notes that if the data are missing completely at random, this may be a viable approach.\n",
    "\n",
    "8.5 Imputation Methods\n",
    "Imputation does beg the question of how much missing data are too much to impute? Although not a general rule in any sense, 20% missing data within a column might be a good “line of dignity” to observe. Of course, this depends on the situation and the patterns of missing values in the training set.\n",
    "\n",
    "__It is also important to consider that imputation is probably the first step in any preprocessing sequence. Imputing qualitative predictors prior to creating indicator variables so that the binary nature of the resulting imputations can be preserved is a good idea.__ Also, imputation should usually occur prior to other steps that involve parameter estimation. For example, if centering and scaling is performed on data prior to imputation, the resulting means and standard deviations will inherit the potential biases and issues incurred from data deletion.\n",
    "\n",
    "Linear Methods\n",
    "\n",
    "When a complete predictor shows a strong linear relationship with a predictor that requires imputation, a straightforward linear model may be the best approach. Linear models can be computed very quickly and have very little retained overhead. While a linear model does require complete predictors for the imputation model, this is not a fatal flaw since the model coefficients (i.e., slopes) use all of the data for estimation. Linear regression can be used for a numeric predictor that requires imputation. Similarly, logistic regression is appropriate for a categorical predictor that requires imputation.\n",
    "\n",
    "8.7 Summary\n",
    "\n",
    "Missing values are common occurrences in data. Unfortunately, most predictive modeling techniques cannot handle any missing values. Therefore, this problem must be addressed prior to modeling. Missing data may occur due to random chance or due to a systematic cause. Understanding the nature of the missing values can help to guide the decision process about how best to remove or impute the data.\n",
    "\n",
    "One of the best ways to understand the amount and nature of missing values is through an appropriate visualization. For smaller data sets, a heatmap or co-occurrence plot is helpful. Larger data sets can be visualized by plotting the first two scores from a PCA model of the missing data indicator matrix.\n",
    "\n",
    "Once the severity of missing values is known, then a decision needs to be made about how to address these values. When there is a severe amount of missing data within a predictor or a sample, it may be prudent to remove the problematic predictors or samples. Alternatively, if the predictors are qualitative, missing values could be encoded with a category of “missing”.\n",
    "\n",
    "For small to moderate amounts of missingness, the values can be imputed. There are many types of imputation techniques. A few that are particularly useful when the ultimate goal is to build a predictive model are k-nearest neighbors, bagged trees, and linear models. Each of these methods have a relatively small computation footprint and can be computed quickly which enables them to be included in the resampling process of predictive modeling.\n",
    "\n",
    "10.1 Goals of Feature Selection\n",
    "\n",
    "Here we would like to refocus the motivations for removing predictors from a model. The primary motivations should be to either mitigate a specific problem in the interplay between predictors and a model, or to reduce model complexity. For example:\n",
    "\n",
    "Some models, notably support vector machines and neural networks, are sensitive to irrelevant predictors. As will be shown below, superfluous predictors can sink predictive performance in some situations.\n",
    "\n",
    "Other models like linear or logistic regression are vulnerable to correlated predictors (see Chapter 6). Removing correlated predictors will reduce multicollinearity and thus enable these types of models to be fit.\n",
    "\n",
    "Even when a predictive model is insensitive to extra predictors, it makes good scientific sense to include the minimum possible set that provides acceptable results. In some cases, removing predictors can reduce the cost of acquiring data or improve the throughput of the software used to make predictions.\n",
    "\n",
    "The working premise here is that it is generally better to have fewer predictors in a model. For the remaining chapters, the goal of feature selection will be re-framed to\n",
    "\n",
    "Reduce the number of predictors as far as possible without compromising predictive performance.\n",
    "\n",
    "There are a variety of methods to reduce the predictor set. The next section provides an overview of the general classes of feature selection techniques.\n",
    "\n",
    "11.3 Recursive Feature Elimination\n",
    "\n",
    "As previously noted, recursive feature elimination (RFE, Guyon et al. (2002)) is basically a backward selection of the predictors. This technique begins by building a model on the entire set of predictors and computing an importance score for each predictor. The least important predictor(s) are then removed, the model is re-built, and importance scores are computed again. In practice, the analyst specifies the number of predictor subsets to evaluate as well as each subset’s size. Therefore, the subset size is a tuning parameter for RFE. The subset size that optimizes the performance criteria is used to select the predictors based on the importance rankings. The optimal subset is then used to train the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4f378-5d23-4420-85ee-ed4afe3687b6",
   "metadata": {},
   "source": [
    "Maybe use scikit-learn’s Pipeline method to build the model. It employs a standard scalar that normalizes each feature (because they all have different units, and therefore scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9a37e-0ca1-42be-84a0-96a7f83b53f4",
   "metadata": {},
   "source": [
    "Regression\n",
    "\n",
    "Regression is a supervised machine learning technique used to model the relationship of one or more features or independent variables (one=simple regression, more = multiple regression)\n",
    "to one or more target or dependent variables,(one = univariate regression, more = multivariate regression). The variables are represented by continuous data. \n",
    "\n",
    "A regression algorithm attempts to find the function that best 'mimics' or 'models' the relationship between independent features and dependent target variables. The algorithm does this by finding the line (simple regression) or plane (multiple regression) that minimizes the errors in our predictions when compared to the labeled data. After acquiring that function, it can be used to make predictions on new observations when they become available; we can simply run these new values of the independent variables through the function for each observation to predict the dependent target variables. \n",
    "\n",
    "The algorithm attempts to find the \"best\" choices of values for the parameters, which in a linear regression model are the coefficients in order to make the formula as \"accurate\" as possible, i.e. minimize the error. There are different ways to define the error, but whichever evaluation metric we select, the algorithm finds the line of best fit by identifying the parameters that minimize that error. \n",
    "\n",
    "Once estimated, the parameters (intercept and coefficients) allow the value of the target variable to be obtained from the values of the feature variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629994eb-f959-4ef8-9838-99aa347f77ae",
   "metadata": {},
   "source": [
    "# in case I want to approximate neighborhood or city locations\n",
    "Accuracy is the tendency of your measurements to agree with the true values. Precision is the degree to which your measurements pin down an actual value. The question is about an interplay of accuracy and precision.\n",
    "\n",
    "As a general principle, you don't need much more precision in recording your measurements than there is accuracy built into them. Using too much precision can mislead people into believing the accuracy is greater than it really is.\n",
    "\n",
    "Generally, when you degrade precision--that is, use fewer decimal places--you can lose some accuracy. But how much? It's good to know that the meter was originally defined (by the French, around the time of their revolution when they were throwing out the old systems and zealously replacing them by new ones) so that ten million of them would take you from the equator to a pole. That's 90 degrees, so one degree of latitude covers about 10^7/90 = 111,111 meters. (\"About,\" because the meter's length has changed a little bit in the meantime. But that doesn't matter.) Furthermore, a degree of longitude (east-west) is about the same or less in length than a degree of latitude, because the circles of latitude shrink down to the earth's axis as we move from the equator towards either pole. Therefore, it's always safe to figure that the sixth decimal place in one decimal degree has 111,111/10^6 = about 1/9 meter = about 4 inches of precision.\n",
    "\n",
    "Accordingly, if your accuracy needs are, say, give or take 10 meters, than 1/9 meter is nothing: you lose essentially no accuracy by using six decimal places. If your accuracy need is sub-centimeter, then you need at least seven and probably eight decimal places, but more will do you little good.\n",
    "\n",
    "Thirteen decimal places will pin down the location to 111,111/10^13 = about 1 angstrom, around half the thickness of a small atom.\n",
    "\n",
    "Using these ideas we can construct a table of what each digit in a decimal degree signifies:\n",
    "\n",
    "The sign tells us whether we are north or south, east or west on the globe.\n",
    "A nonzero hundreds digit tells us we're using longitude, not latitude!\n",
    "The tens digit gives a position to about 1,000 kilometers. It gives us useful information about what continent or ocean we are on.\n",
    "The units digit (one decimal degree) gives a position up to 111 kilometers (60 nautical miles, about 69 miles). It can tell us roughly what large state or country we are in.\n",
    "The first decimal place is worth up to 11.1 km: it can distinguish the position of one large city from a neighboring large city.\n",
    "The second decimal place is worth up to 1.1 km: it can separate one village from the next.\n",
    "The third decimal place is worth up to 110 m: it can identify a large agricultural field or institutional campus.\n",
    "_The fourth decimal place is worth up to 11 m: it can identify a parcel of land. It is comparable to the typical accuracy of an uncorrected GPS unit with no interference._\n",
    "The fifth decimal place is worth up to 1.1 m: it distinguish trees from each other. Accuracy to this level with commercial GPS units can only be achieved with differential correction.\n",
    "The sixth decimal place is worth up to 0.11 m: you can use this for laying out structures in detail, for designing landscapes, building roads. It should be more than good enough for tracking movements of glaciers and rivers. This can be achieved by taking painstaking measures with GPS, such as differentially corrected GPS.\n",
    "The seventh decimal place is worth up to 11 mm: this is good for much surveying and is near the limit of what GPS-based techniques can achieve.\n",
    "The eighth decimal place is worth up to 1.1 mm: this is good for charting motions of tectonic plates and movements of volcanoes. Permanent, corrected, constantly-running GPS base stations might be able to achieve this level of accuracy.\n",
    "The ninth decimal place is worth up to 110 microns: we are getting into the range of microscopy. For almost any conceivable application with earth positions, this is overkill and will be more precise than the accuracy of any surveying device.\n",
    "Ten or more decimal places indicates a computer or calculator was used and that no attention was paid to the fact that the extra decimals are useless. Be careful, because unless you are the one reading these numbers off the device, this can indicate low quality processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6bbb70-3e18-4b41-bd99-b5ba9fdf75e3",
   "metadata": {},
   "source": [
    "# Concerning Bathrooms\n",
    "https://www.sfgate.com/local/article/The-weird-story-behind-why-there-are-so-many-15937140.php\n",
    "\n",
    "When most homes in the Victorian era (1837-1901) were constructed, there was no toilet in the home at all, since most people would have still used an outhouse and/or chamber pots. Indoor plumbing was just becoming common, so Bay Area residents may have had their sinks and tubs installed years before they had added an indoor toilet. Once they were able to add the toilet, it may have made more sense to convert a nearby closet into a toilet room rather than build into the existing bathroom.\n",
    "\n",
    "More likely, though, it stemmed from the Victorian era’s new obsession with hygiene. “The idea was separating where you clean yourself and where you defecate,” Spindler said. “They would have thought it was highly uncleanly to defecate in the same room you take a bath in and shave in.”\n",
    "\n",
    "Rob Thomson, president of the Victorian Alliance of San Francisco, said Victorian era residents were eager adopters of new technology, and in the residential sphere there was no bigger story in the second half of the 19th century than indoor plumbing. “This trend in residential buildings was given a boost by the advent of consistent municipal water supplies and sewage systems — these were the fiber data networks of the 1870s,” he said.\n",
    "\n",
    "Orange County real estate agents must specify the number of bedrooms and bathrooms in a house listed in the California Regional Multiple Listing Service  (CRMLS). And bedrooms have to be expressed in whole numbers.\n",
    "\n",
    "There is no such thing as a 3 ½ bedroom house in Southern California.\n",
    "\n",
    "Bathrooms, on the other hand, come in four different sizes.\n",
    "\n",
    "Full, three-quarter, one-half and one-quarter. And in the current system, Orange County agents are required to specify how many of each size a home has. Even if that number is zero.  \n",
    "\n",
    "Not to get too far into the weeds here, but many buyers have been misled by this information fueled by misinterpretation of the numbers. So here’s how the numbers work.\n",
    "\n",
    "Number of Bathrooms: CRMLS instructions specify that “a full bathroom typically has four elements — a sink, a toilet, a bathtub and a shower head (either in the tub or a stall).”\n",
    "\n",
    "Three-quarter bathrooms typically have three elements — a sink, a toilet and a shower. Half bathrooms typically have two elements — a sink and a toilet.\n",
    "\n",
    "And one quarter bath rooms typically have one element – a sink or a toilet.\n",
    "\n",
    "So let’s break this down — Orange County style. A full bath is a bathroom that has all four elements, regardless if the shower is separate from the tub. Let me repeat that. By definition, a full bath is both a bathroom with a sink, a toilet, a tub and a separate shower and a bathroom that has a sink, a toilet, and a bathtub with a shower head.\n",
    "\n",
    "A three-quarter bathroom has a sink, a toilet, and a shower — just no bath tub. It might qualify as a three-quarter bath if it had a sink, a toilet, and a tub with no shower head. But the instructions don’t cover that scenario, so I’m winging it here.\n",
    "\n",
    "A half bath is a sink and a toilet — what most of us would call a powder room.\n",
    "\n",
    "I don’t think I’ve ever seen an one-quarter bath, except in a garage that had a urinal. That might actually count as an one-eighth bathroom, as most females might find it difficult to use.\n",
    "\n",
    "https://nypost.com/2021/03/05/bathroom-doesnt-have-walls-in-900k-home-for-sale/\n",
    "HILARIOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a8143-63c5-493c-8c89-0b4f93711900",
   "metadata": {},
   "source": [
    "## Lot Sizes\n",
    "https://www.cpappraisal.com/lot-size-impact-property-value/\n",
    "\n",
    "Homeowners who have larger lots than other lots in their area want to know if the value of their home would be substantially more. If a specific lot is approximately 3,000 to 5,000 square feet larger than other lots, it should have an appraised value that is greater. The more important questions is by how much?\n",
    "\n",
    "Researching other homes with larger lots and then comparing them to homes with smaller sized lots, will give you a pretty good idea of what the market is agreeable to pay. In some cases there is a significant difference in price depending on the size of the lot, other cases the difference may not be very much at all. So, here are some important points that should be taken into consideration:\n",
    "\n",
    "The Location of Your Space:\n",
    "\n",
    "If the extra square footage is located in the front of your home, chances are it’s not going to be very valuable. Most buyers prefer more square footage in the backyard vs a street view.\n",
    "\n",
    "The Actual Percentage Impact Can Dictate Price:\n",
    "\n",
    "There are some areas or neighborhoods that an extra 5,000 square feet would make absolutely no difference whatsoever. If the average residence is sitting on 3 acres, 5,000 square feet is a mere 4% which is not going to be significant to potential buyers.\n",
    "\n",
    "Location – Location:\n",
    "\n",
    "It’s no secret that almost all aspects of real estate value are tied to location – people will always be willing to pay more for property that is in desirable areas. This is as true for “micro” location as it is for “macro”. If your extra large lot is sitting on a very busy street, most buyers are going to have an adverse reaction to the size. Many buyers will actually pay less for that kind of condition. On the other hand, if you are on a small, very quite street or out in the country a buyer is more likely to offer more for that added space.\n",
    "\n",
    "New Construction Premiums:\n",
    "\n",
    "Most builders will charge a great deal more for larger lots, but in the long run, the resale market might not want to pay that same premium. If you paid $100,000 for a substantial lot, you think the future buyer would be willing to pay the same. Unfortunately, that’s not the case and the market may not be willing to do so a few years down the road.\n",
    "\n",
    "Is The Space Useful:\n",
    "\n",
    "The Advantage or convenience of the lot may also dictate its value. If that added lot space is not usable due to zoning, easements, vernal pools or other issues, the lot is not going to hold much value.\n",
    "\n",
    "Double The Value:\n",
    "\n",
    "Some homeowners believe that their lot should be worth more in value when the size increases. Unfortunately, that’s just not true. If you purchase an extra-large dress, would you be willing to pay twice as much as a smaller dress? Of course not. If one acre sells for $100,000 wouldn’t 4 acres sell for $400,000? Additional acres will actually have a lower value than the initial acre. It’s called diminishing returns.\n",
    "\n",
    "In Conclusion:\n",
    "\n",
    "There a many variables in extra lot sizes or extra acres. Location, convenience and what the true value might be years from now. Keep in mind, picking up more square footage in hopes of increasing your home value, might not have the financial show you were hoping for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b8c68-1e88-49f3-a631-5a8a4e90c7aa",
   "metadata": {},
   "source": [
    "Also, the rooms that are not meant for sleeping can be no less than 70 square feet.  So, a two bedroom home with a livings room should be no less than 310 square feet.  The minimum size of the home only goes up the more rooms the house has. If you start looking at houses that have more than one room, the minimum size of the home starts to go up.  In studio homes, the minimum square feet is 120.  However, with multiple rooms, each livable room has to be at least 120 square feet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee68e-b6b0-49cf-99ca-ad51383ac90b",
   "metadata": {},
   "source": [
    "https://www.latimes.com/homeless-housing/story/2021-09-17/what-just-happened-with-single-family-zoning-in-california\n",
    "\n",
    "Single-family zoning refers to a residential area where only one housing unit can be built on a given parcel of land. Think of the Southern California suburban staple of a home with no shared walls — in other words, not a duplex, triplex or multiunit complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb575d6b-8182-4457-8e85-3760e9721b36",
   "metadata": {},
   "source": [
    "# SERIOUS PLANNING. Ideas to Implement \n",
    "\n",
    "- Create three separate data frames for each count, LA, Ventura, and Orange. Look at the outliers in each of those and remove them individually, then merge the data frame. \n",
    "- Worrying about the cities or neighborhoods at this juncture (at my low-skill level) is impractical. There will be far too many unique values, as an example, there are 88 cities in Los Angeles (jsyk: 34 in Orange, 18 in Ventura)\n",
    "- Because of the low impact on the dataframe, there is no reason whatsoever to consider imputing the values, UNLESS, this is done after the counties are subdivided and the median is calculated for each, seeing how this wouldn't be impacted by outliers. Nevertheless, Zillow has a lot of problems with their data. \n",
    "- When homes are listed as having 0 baths, this is highly suspect. 0 bedrooms is acceptable, as they could be classified as studios. 0 baths would perhaps make sense for homes from the Victorian Era, but this would also interferece with the model. Effectively, all homes before 1901 should be removed. Hopefully there won't be many, so as to be justifiable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25235fc0-8d9b-41a1-a2c1-28d752669e77",
   "metadata": {},
   "source": [
    "County Tax Rate: Los Angeles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
